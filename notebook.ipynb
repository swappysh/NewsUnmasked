{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import os, json\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"generative-newsai/news-unmasked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset[\"train\"]\n",
    "test_set = dataset[\"test\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the processed train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "!gdown https://drive.google.com/uc?id=1OUj56f7gX7_C-xQE6YkVBpJICVNcDsNl\n",
    "!UNZIP_DISABLE_ZIPBOMB_DETECTION=TRUE unzip processed_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset from https://drive.google.com/file/d/1sE2kae6-Uj7B0ThIX8bTfKdCqq9aKp4n/view?usp=sharing\n",
    "\n",
    "from datasets import load_from_disk\n",
    "train_set = load_from_disk(\"./processed_data\")\n",
    "train_train_set, train_val_set = torch.utils.data.random_split(train_set, \n",
    "                                                               [int(0.8 * len(train_set)),\n",
    "                                                                len(train_set) - int(0.8 * len(train_set))])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.__len__(), test_set.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.features, test_set.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[random.choice(range(test_set.__len__()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many test headlines have [MASK] in them?\n",
    "mask_token = \"[MASK]\"\n",
    "test_with_mask = [x for x in test_set[\"headline\"] if mask_token in x]\n",
    "print(f\"{len(test_with_mask)} out of {len(test_set)} test headlines have [MASK] in them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tests without [MASK] in them\n",
    "test_without_mask = [x for x in test_set[\"headline\"] if mask_token not in x]\n",
    "print(f\"{len(test_without_mask)} out of {len(test_set)} test headlines do not have [MASK] in them\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Headlines with no mask in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_without_mask[random.choice(range(len(test_without_mask)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = np.unique(test_set[\"section\"])\n",
    "print(\"Unique sections in the test dataset: \", len(sections))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: We have image, section and a masked headline. We need to predict the appropriate word for the masked headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many train headlines have [MASK] in them?\n",
    "train_with_mask = [x for x in train_set[\"headline\"] if mask_token in x]\n",
    "print(f\"{len(train_with_mask)} out of {len(train_set)} train headlines have [MASK] in them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = np.unique(train_set[\"section\"])\n",
    "print(\"Unique sections in the train dataset: \", len(sections))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: Not all the test entries have [MASK] token in them and there are 24 unique sections in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot section count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Plot np unique with counts\n",
    "z_train = np.unique(train_set[\"section\"], return_counts=True)\n",
    "z_test = np.unique(test_set[\"section\"], return_counts=True)\n",
    "\n",
    "# np unique returns sorted unique elements\n",
    "assert np.array_equal(z_train[0], z_test[0])\n",
    "\n",
    "# plot grouped bar chart\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(z_train[0]))\n",
    "width = 0.35\n",
    "rects1 = ax.bar(x - width / 2, z_train[1], width, label=\"Train\")\n",
    "rects2 = ax.bar(x + width / 2, z_test[1]*(train_set.__len__()/test_set.__len__()), width, \n",
    "                label=\"Scaled Up Test\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Count by Section\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(z_train[0])\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: We have 24 unique sections with Well, Education, Global Business, Your Money, Economy and Automobiles having lowest occurence. Train dataset and test dataset follow similar pattern for section distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot headline length by section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Group headline_length_per_section by section\n",
    "from itertools import groupby\n",
    "\n",
    "def groupby_section(dataset):\n",
    "    ln_per_section = list(zip(dataset[\"section\"], [len(headline) for headline in dataset[\"headline\"]]))\n",
    "    ln_per_section = sorted(ln_per_section, key=lambda x: x[0])\n",
    "    ln_per_section = [(key, list(group)) for key, group in groupby(ln_per_section, lambda x: x[0])]\n",
    "    ln_per_section = [(key, [x[1] for x in group]) for key, group in ln_per_section]\n",
    "    ln_stats_per_section = [(key, np.mean(group), np.std(group)) for key, group in ln_per_section]\n",
    "    return ln_stats_per_section\n",
    "\n",
    "train_ln_stats_per_section = groupby_section(train_set)\n",
    "test_ln_stats_per_section = groupby_section(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Plot train_ln_stats_per_section and test_ln_stats_per_section as grouped bar chart with error bars\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(train_ln_stats_per_section))\n",
    "width = 0.35\n",
    "rects1 = ax.bar(x - width / 2, [x[1] for x in train_ln_stats_per_section], width, \n",
    "                yerr=[x[2] for x in train_ln_stats_per_section], label=\"Train\")\n",
    "rects2 = ax.bar(x + width / 2, [x[1] for x in test_ln_stats_per_section], width, \n",
    "                yerr=[x[2] for x in test_ln_stats_per_section], label=\"Test\")\n",
    "ax.set_ylabel(\"Headline Length\")\n",
    "ax.set_title(\"Headline Length by Section\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([x[0] for x in train_ln_stats_per_section])\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: No outliers in headline length. We can see that the length of the headline is similar across all sections and lies between ~30 to ~80 characters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample images\n",
    "\n",
    "Uncomment to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "imgs = []\n",
    "for i, section in enumerate(tqdm(sections)):\n",
    "    indices = random.choices(np.where(np.asarray(train_set[\"section\"]) == section)[0], k=5)\n",
    "    imgs.append(train_set[indices][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Plot images for each section\n",
    "fig, axs = plt.subplots(6, 4, figsize=(20, 20))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(imgs[i][random.randint(0, 4)])\n",
    "    ax.set_title(sections[i])\n",
    "    ax.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "[img.size for img in train_set[random.sample(range(train_set.__len__()), 10)][\"image\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: Visualized Images have nytimes labeel showing what year it was printed, and sizes vary across images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deberta-Base-Uncased\n",
    "\n",
    "Based on the sample notebook provided --> https://huggingface.co/datasets/generative-newsai/news-unmasked/blob/main/.extras/helper_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "model_name = \"mlcorelib/deberta-base-uncased\"\n",
    "unmasker = pipeline('fill-mask', model=model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check first 5 rows of the Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Check first 5 rows of the test dataset\n",
    "sample_data = test_set[:5]\n",
    "for k, v in sample_data.items():\n",
    "    print(\"-\"*50)\n",
    "    print(k)\n",
    "    print(*v, sep=\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unmask the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "all_masked_words = []\n",
    "test_subset = Subset(test_set, range(100))\n",
    "for each_dict in tqdm(test_subset):\n",
    "    sentence = each_dict['headline']  # Get the sentence from the dictionary\n",
    "    image_id = each_dict['image_id']  # Get the image_id from the dictionary\n",
    "    if \"[MASK]\" in sentence: # See if it has a [MASK] in headline\n",
    "        result = unmasker(sentence)  # Unmask the sentence\n",
    "\n",
    "        # Make a list of indices where [MASK] is present in the sentence\n",
    "        # If there are more than one [MASK] in the sentence, then add them as separate entries in the result list\n",
    "        indices = [i for i, x in enumerate(sentence.split()) if x == \"[MASK]\"]\n",
    "        if len(indices) > 1:\n",
    "            masked_word_idx_list = []\n",
    "            for i, each_result in enumerate(result):\n",
    "                # Get the top scoring word\n",
    "                top_word = each_result[0]['token_str']\n",
    "                all_masked_words.append([image_id, indices[i], top_word])\n",
    "        else:\n",
    "            all_masked_words.append([image_id, indices[0], result[0]['token_str']])\n",
    "\n",
    "final_masked_words = [l[0] for l in all_masked_words]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print first 5 rows of the masked words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "print(final_masked_words[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results as a dataframe and print first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Save the results in a dataframe with column name id,token_index,token\n",
    "df = pd.DataFrame(all_masked_words, columns=['id', 'token_index', 'token'])\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataframe as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "df.to_csv('sample_result.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Language Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea: Caption the image\n",
    "Ask LLM caption + mask sentence. Predict masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide train_set in train and validation\n",
    "train_train_set, train_val_set = torch.utils.data.random_split(train_set, \n",
    "                                                               [int(0.8 * len(train_set)),\n",
    "                                                                len(train_set) - int(0.8 * len(train_set))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_train_set), len(train_val_set), len(train_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Captioning using hf pipelines"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VIT-GPT2-COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_id = 0 if torch.cuda.is_available() else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from transformers import pipeline\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "\n",
    "captioner = pipeline(\"image-to-text\",model=\"ydshieh/vit-gpt2-coco-en\", device=device_id)\n",
    "captioner(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n",
    "## [{'generated_text': 'two birds are standing next to each other '}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Try captioner on one of the images in the validation set\n",
    "rouge = ROUGEScore(use_stemmer=True)\n",
    "sample = train_val_set[random.randint(0, len(train_val_set))]\n",
    "\n",
    "caption = captioner(sample[\"image\"], max_new_tokens=25)\n",
    "rouge_score = rouge(caption[0]['generated_text'], sample[\"headline\"])\n",
    "plt.imshow(sample[\"image\"])\n",
    "print(\"generated_text: \" + caption[0]['generated_text'], \n",
    "      \"section: \" + sample[\"section\"],  \n",
    "      \"headline: \" + sample[\"headline\"], \n",
    "      \"rouge1_fmeasure: \" + str(rouge_score['rouge1_fmeasure']),\n",
    "      \"rouge2_fmeasure: \" + str(rouge_score['rouge2_fmeasure']),\n",
    "      \"rougeL_fmeasure: \" + str(rouge_score['rougeL_fmeasure']),\n",
    "      \"rougeLsum_fmeasure: \" + str(rouge_score['rougeLsum_fmeasure']),\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# rouge1_fmeasure across validation subset\n",
    "rouge1_fmeasure = []\n",
    "train_val_subset = Subset(train_val_set, range(100))\n",
    "for data in tqdm(train_val_subset):\n",
    "    caption = captioner(data[\"image\"], max_new_tokens=25)\n",
    "    rouge_score = rouge(caption[0]['generated_text'], data[\"headline\"])\n",
    "    rouge1_fmeasure.append(rouge_score['rouge1_fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "np.mean(rouge1_fmeasure), np.std(rouge1_fmeasure)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLIP image captioning large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captioner = pipeline(\"image-to-text\",model=\"Salesforce/blip-image-captioning-large\", device=device_id)\n",
    "captioner(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n",
    "## VIT-GPT2 output: ## [{'generated_text': 'two birds are standing next to each other '}]\n",
    "## BLIP output: [{'generated_text': 'there are two parrots that are standing next to each other'}]\n",
    "## Observations: BLIP output seems more detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Try captioner on one of the images in the validation set\n",
    "rouge = ROUGEScore(use_stemmer=True)\n",
    "sample = train_val_set[random.randint(0, len(train_val_set))]\n",
    "\n",
    "caption = captioner(sample[\"image\"], max_new_tokens=25)\n",
    "rouge_score = rouge(caption[0]['generated_text'], sample[\"headline\"])\n",
    "plt.imshow(sample[\"image\"])\n",
    "print(\"generated_text: \" + caption[0]['generated_text'], \n",
    "      \"section: \" + sample[\"section\"],  \n",
    "      \"headline: \" + sample[\"headline\"], \n",
    "      \"rouge1_fmeasure: \" + str(rouge_score['rouge1_fmeasure']),\n",
    "      \"rouge2_fmeasure: \" + str(rouge_score['rouge2_fmeasure']),\n",
    "      \"rougeL_fmeasure: \" + str(rouge_score['rougeL_fmeasure']),\n",
    "      \"rougeLsum_fmeasure: \" + str(rouge_score['rougeLsum_fmeasure']),\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# rouge1_fmeasure across validation subset\n",
    "rouge1_fmeasure = []\n",
    "train_val_subset = Subset(train_val_set, range(100))\n",
    "for data in tqdm(train_val_subset):\n",
    "    caption = captioner(data[\"image\"], max_new_tokens=25)\n",
    "    rouge_score = rouge(caption[0]['generated_text'], data[\"headline\"])\n",
    "    rouge1_fmeasure.append(rouge_score['rouge1_fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "np.mean(rouge1_fmeasure), np.std(rouge1_fmeasure)\n",
    "\n",
    "# VIT-GPT2 scores: (0.06309018, 0.07374029)\n",
    "# BLIP scores: (0.06341785, 0.069144554)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Inference takes longer than VIT-GPT2 (4:47 as compared to 1:09 for inital 100 samples). Eye balling the results, BLIP image captioning large seems to be more specific to the image. VIT-GPT2 seems to be more generic. rouge score is slightly better for BLIP image captioning large for the first 100 samples. Std dev is also lower for BLIP image captioning large.\n",
    "\n",
    "Conclusions:\n",
    "\n",
    "Can try both models and see which gives better results for masked word prediction."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill-Mask Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"distilroberta-base\", device=device_id)\n",
    "unmasker(\"Paris is the <mask> of France.\")\n",
    "\n",
    "# [{'score': 0.7, 'sequence': 'Paris is the capital of France.'},\n",
    "# {'score': 0.2, 'sequence': 'Paris is the birthplace of France.'},\n",
    "# {'score': 0.1, 'sequence': 'Paris is the heart of France.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "sample = train_val_set[random.randint(0, len(train_val_set))]\n",
    "\n",
    "# replace random word with <mask>\n",
    "masked_sample = sample[\"headline\"].split()\n",
    "rand_index = random.randint(0, len(masked_sample)-1)\n",
    "print(f\"rand index: {rand_index}, len of headline: {len(masked_sample)}\")\n",
    "\n",
    "masked_sample[rand_index] = \"<mask>\"\n",
    "masked_sample = \" \".join(masked_sample)\n",
    "sample[\"headline\"], masked_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "unmasked = unmasker(masked_sample)\n",
    "\n",
    "# compute rouge score with original headline\n",
    "rouge = ROUGEScore(use_stemmer=True)\n",
    "print(\"original headline: \" + sample[\"headline\"], \"section: \" + sample[\"section\"], sep=\"\\n\")\n",
    "for i in range(len(unmasked)):\n",
    "    rouge_score = rouge(unmasked[i]['sequence'], sample[\"headline\"])\n",
    "    print(\"generated_text: \" + unmasked[i]['sequence'],  \n",
    "          \"rouge1_fmeasure: \" + str(rouge_score['rouge1_fmeasure']),\n",
    "          \"rouge2_fmeasure: \" + str(rouge_score['rouge2_fmeasure']),\n",
    "          \"rougeL_fmeasure: \" + str(rouge_score['rougeL_fmeasure']),\n",
    "          \"rougeLsum_fmeasure: \" + str(rouge_score['rougeLsum_fmeasure']),\n",
    "          sep=\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill mask pipeline with caption as prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomMask(input, mask_token=\"<mask>\"):\n",
    "    masked_input = input.split()\n",
    "    rand_index = random.randint(0, len(masked_input)-1)\n",
    "    masked_input[rand_index] = mask_token\n",
    "    masked_input = \" \".join(masked_input)\n",
    "    return masked_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script false --no-raise-error\n",
    "\n",
    "sample = train_val_set[random.randint(0, len(train_val_set))]\n",
    "masked_sample = randomMask(sample[\"headline\"])\n",
    "sample[\"headline\"], masked_sample\n",
    "\n",
    "PADDING = \"[ pad ]\"\n",
    "caption = captioner(sample[\"image\"], max_new_tokens=25)\n",
    "unmasked = unmasker(masked_sample)\n",
    "prompted_unmasked = unmasker(caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "\n",
    "plt.imshow(sample[\"image\"])\n",
    "print(\"original headline: \" + sample[\"headline\"], \"section: \" + sample[\"section\"], sep=\"\\n\")\n",
    "print(\"caption: \" + caption[0]['generated_text'], \"masked_sample: \" + masked_sample, \n",
    "      \"unmasked: \" + unmasked[0]['sequence'], \n",
    "      \"prompted_unmasked: \" + prompted_unmasked[0]['sequence'].split(PADDING)[1],\n",
    "        sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# What percentage improves with prompting?\n",
    "\n",
    "train_val_subset = Subset(train_val_set, range(100))\n",
    "rouge1_fmeasure_prompted = []\n",
    "rouge1_fmeasure_unprompted = []\n",
    "for data in tqdm(train_val_subset):\n",
    "    caption = captioner(data[\"image\"], max_new_tokens=25)\n",
    "    \n",
    "    masked_sample = randomMask(data[\"headline\"])\n",
    "    \n",
    "    unmasked = unmasker(masked_sample)\n",
    "    prompted_unmasked = unmasker(caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "    rouge1_fmeasure_prompted.append(rouge(prompted_unmasked[0]['sequence'].split(PADDING)[1], \n",
    "                                          data[\"headline\"])['rouge1_fmeasure'])\n",
    "    rouge1_fmeasure_unprompted.append(rouge(unmasked[0]['sequence'],\n",
    "                                            data[\"headline\"])['rouge1_fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "rouge1_fmeasure_prompted = np.array(rouge1_fmeasure_prompted)\n",
    "rouge1_fmeasure_unprompted = np.array(rouge1_fmeasure_unprompted)\n",
    "print(np.mean(rouge1_fmeasure_prompted), np.std(rouge1_fmeasure_prompted), \n",
    "      np.mean(rouge1_fmeasure_unprompted), np.std(rouge1_fmeasure_unprompted))\n",
    "print(np.mean(rouge1_fmeasure_prompted > rouge1_fmeasure_unprompted))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_set.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "all_masked_words = []\n",
    "# train_val_loader = DataLoader(train_val_set, batch_size=1, shuffle=False)\n",
    "for each_dict in tqdm(train_val_set):\n",
    "    original = each_dict['headline'].split(' ')  # Get the original sentence from the dictionary\n",
    "    image = each_dict['image']  # Get the image from the dictionary\n",
    "    sentence = each_dict['masked_headline']  # Get the masked_headline from the dictionary\n",
    "    image_id = each_dict['image_id']  # Get the image_id from the dictionary\n",
    "    if \"[MASK]\" in sentence: # See if it has a [MASK] in headline\n",
    "        sentence = sentence.replace(\"[MASK]\", \"<mask>\")\n",
    "        caption = captioner(image, max_new_tokens=25)\n",
    "        result = unmasker(caption[0]['generated_text'] + PADDING + sentence)\n",
    "        \n",
    "        # result = prompted_unmasked[0]['sequence'].split(PADDING)[1]\n",
    "\n",
    "        # Make a list of indices where [MASK] is present in the sentence\n",
    "        # If there are more than one [MASK] in the sentence, then add them as separate entries in the result list\n",
    "        indices = [i for i, x in enumerate(sentence.split()) if x == \"<mask>\"]\n",
    "        if len(indices) > 1:\n",
    "            masked_word_idx_list = []\n",
    "            for i, each_result in enumerate(result):\n",
    "                # Get the top scoring word\n",
    "                top_word = each_result[0]['token_str']\n",
    "                all_masked_words.append([image_id, indices[i], top_word, original[indices[i]]])\n",
    "        else:\n",
    "            all_masked_words.append([image_id, indices[0], result[0]['token_str'], original[indices[0]]])\n",
    "\n",
    "final_masked_words = [l[0] for l in all_masked_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Compute cosine similarity using en_core_web_lg\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "cosine_sim_threshold = 0.5\n",
    "num_correct = 0\n",
    "for each_masked_word in tqdm(all_masked_words):\n",
    "    # Get the top scoring word\n",
    "    top_word = each_masked_word[2]\n",
    "    # Get the original word\n",
    "    original_word = each_masked_word[3]\n",
    "    # Compute cosine similarity\n",
    "    similarity = nlp(top_word).similarity(nlp(original_word))\n",
    "    if similarity >= cosine_sim_threshold:\n",
    "        num_correct += 1\n",
    "\n",
    "accuracy = num_correct / len(all_masked_words) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "mask_token = \"[MASK]\"\n",
    "test_indices_with_mask = [i for i, x in enumerate(test_set[\"headline\"]) if mask_token in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "all_masked_words = []\n",
    "test_subset = Subset(test_set, test_indices_with_mask)\n",
    "for each_dict in tqdm(test_subset):\n",
    "    image = each_dict['image']  # Get the image from the dictionary\n",
    "    sentence = each_dict['headline']  # Get the sentence from the dictionary\n",
    "    image_id = each_dict['image_id']  # Get the image_id from the dictionary\n",
    "    if \"[MASK]\" in sentence: # See if it has a [MASK] in headline\n",
    "        sentence = sentence.replace(\"[MASK]\", \"<mask>\")\n",
    "        caption = captioner(image, max_new_tokens=25)\n",
    "        result = unmasker(caption[0]['generated_text'] + PADDING + sentence)\n",
    "        \n",
    "        # result = prompted_unmasked[0]['sequence'].split(PADDING)[1]\n",
    "\n",
    "        # Make a list of indices where [MASK] is present in the sentence\n",
    "        # If there are more than one [MASK] in the sentence, then add them as separate entries in the result list\n",
    "        indices = [i for i, x in enumerate(sentence.split()) if x == \"<mask>\"]\n",
    "        if len(indices) > 1:\n",
    "            masked_word_idx_list = []\n",
    "            for i, each_result in enumerate(result):\n",
    "                # Get the top scoring word\n",
    "                top_word = each_result[0]['token_str']\n",
    "                all_masked_words.append([image_id, indices[i], top_word])\n",
    "        else:\n",
    "            all_masked_words.append([image_id, indices[0], result[0]['token_str']])\n",
    "\n",
    "final_masked_words = [l[0] for l in all_masked_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "len(final_masked_words), np.unique(final_masked_words).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "df = pd.DataFrame(all_masked_words, columns=['id', 'token_index', 'token'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "df.to_csv('sample_result.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Mask with caption + section as prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "sample = train_val_set[random.randint(0, len(train_val_set))]\n",
    "masked_sample = randomMask(sample[\"headline\"])\n",
    "sample[\"headline\"], masked_sample\n",
    "\n",
    "PADDING = \"[ pad ]\"\n",
    "caption = captioner(sample[\"image\"], max_new_tokens=25)\n",
    "unmasked = unmasker(masked_sample)\n",
    "prompted_unmasked = unmasker(caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "section_prompted_unmasked = unmasker(sample[\"section\"] + PADDING + \n",
    "                                     caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "\n",
    "plt.imshow(sample[\"image\"])\n",
    "print(\"original headline: \" + sample[\"headline\"], \"section: \" + sample[\"section\"], sep=\"\\n\")\n",
    "print(\"caption: \" + caption[0]['generated_text'], \"masked_sample: \" + masked_sample, \n",
    "      \"unmasked: \" + unmasked[0]['sequence'], \n",
    "      \"prompted_unmasked: \" + prompted_unmasked[0]['sequence'].split(PADDING)[1],\n",
    "      \"section_prompted_unmasked: \" + section_prompted_unmasked[0]['sequence'].split(PADDING)[2],\n",
    "        sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# What percentage improves with adding section as a prompt as well?\n",
    "\n",
    "train_val_subset = Subset(train_val_set, range(100))\n",
    "rouge1_fmeasure_prompted = []\n",
    "rouge1_fmeasure_unprompted = []\n",
    "rouge1_fmeasure_section_prompted = []\n",
    "for data in tqdm(train_val_subset):\n",
    "    caption = captioner(data[\"image\"], max_new_tokens=25)\n",
    "    \n",
    "    masked_sample = randomMask(data[\"headline\"])\n",
    "    \n",
    "    unmasked = unmasker(masked_sample)\n",
    "    prompted_unmasked = unmasker(caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "    section_prompted_unmasked = unmasker(data[\"section\"] + PADDING +\n",
    "                                            caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "    rouge1_fmeasure_prompted.append(rouge(prompted_unmasked[0]['sequence'].split(PADDING)[1], \n",
    "                                          data[\"headline\"])['rouge1_fmeasure'])\n",
    "    rouge1_fmeasure_unprompted.append(rouge(unmasked[0]['sequence'],\n",
    "                                            data[\"headline\"])['rouge1_fmeasure'])\n",
    "    rouge1_fmeasure_section_prompted.append(rouge(section_prompted_unmasked[0]['sequence'].split(PADDING)[2],\n",
    "                                            data[\"headline\"])['rouge1_fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "rouge1_fmeasure_prompted = np.array(rouge1_fmeasure_prompted)\n",
    "rouge1_fmeasure_unprompted = np.array(rouge1_fmeasure_unprompted)\n",
    "rouge1_fmeasure_section_prompted = np.array(rouge1_fmeasure_section_prompted)\n",
    "\n",
    "print(\"prompted mean: \", np.mean(rouge1_fmeasure_prompted), \"\\n\",\n",
    "      \"prompted std: \", np.std(rouge1_fmeasure_prompted), \"\\n\",\n",
    "      \"unprompted mean: \", np.mean(rouge1_fmeasure_unprompted), \"\\n\",\n",
    "      \"unprompted std: \", np.std(rouge1_fmeasure_unprompted), \"\\n\",\n",
    "      \"section_prompted mean: \", np.mean(rouge1_fmeasure_section_prompted), \"\\n\",\n",
    "      \"section_prompted std: \", np.std(rouge1_fmeasure_section_prompted))\n",
    "print(\"prompted > unprompted: \", np.mean(rouge1_fmeasure_prompted > rouge1_fmeasure_unprompted) - \n",
    "      np.mean(rouge1_fmeasure_prompted < rouge1_fmeasure_unprompted), \"\\n\",\n",
    "      \"section_prompted > unprompted: \", np.mean(rouge1_fmeasure_section_prompted > rouge1_fmeasure_unprompted) - \n",
    "      np.mean(rouge1_fmeasure_section_prompted < rouge1_fmeasure_unprompted), \"\\n\",\n",
    "      \"prompted > section_prompted: \", np.mean(rouge1_fmeasure_prompted > rouge1_fmeasure_section_prompted) - \n",
    "      np.mean(rouge1_fmeasure_prompted < rouge1_fmeasure_section_prompted))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Adding section additionaly as the prompt sometimes improves the performance of the model.\n",
    "\n",
    "Nothing conclusive can be said about the performance of the model with section as prompt. It seems to be working better for some sections and worse for others.\n",
    "\n",
    "Maybe can add a description to section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "score1 = -1\n",
    "score2 = -1\n",
    "score3 = -1\n",
    "score4 = -1\n",
    "count = 1000\n",
    "\n",
    "while (score1 <= 0 or score3 <= 0) and (score2 <= 0 or score4 <= 0) and count > 0:\n",
    "      count -= 1\n",
    "      sample = train_val_set[random.randint(0, len(train_val_set))]\n",
    "      masked_sample = randomMask(sample[\"headline\"])\n",
    "      sample[\"headline\"], masked_sample\n",
    "\n",
    "      PADDING = \"[ pad ]\"\n",
    "      caption = captioner(sample[\"image\"], max_new_tokens=25)\n",
    "      unmasked = unmasker(masked_sample)\n",
    "      prompted_unmasked = unmasker(caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "      section_prompted_unmasked = unmasker(sample[\"section\"] + PADDING + \n",
    "                                          caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "      desc_prompted_unmasked = unmasker(\"The context for the caption is {0}.\".format(sample[\"section\"]) + PADDING +\n",
    "                                    caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "      score1 = rouge(desc_prompted_unmasked[0]['sequence'].split(PADDING)[2],\n",
    "                     sample[\"headline\"])['rouge1_fmeasure'] - rouge(unmasked[0]['sequence'],\n",
    "                                                                    sample[\"headline\"])['rouge1_fmeasure']\n",
    "      score2 = rouge(section_prompted_unmasked[0]['sequence'].split(PADDING)[2],\n",
    "                     sample[\"headline\"])['rouge1_fmeasure'] - rouge(unmasked[0]['sequence'],\n",
    "                                                                    sample[\"headline\"])['rouge1_fmeasure']\n",
    "      score3 = rouge(desc_prompted_unmasked[0]['sequence'].split(PADDING)[2],\n",
    "                     sample[\"headline\"])['rouge1_fmeasure'] - rouge(\n",
    "                           prompted_unmasked[0]['sequence'].split(PADDING)[1],\n",
    "                           sample[\"headline\"])['rouge1_fmeasure']\n",
    "      score4 = rouge(section_prompted_unmasked[0]['sequence'].split(PADDING)[2],\n",
    "                        sample[\"headline\"])['rouge1_fmeasure'] - rouge(\n",
    "                              prompted_unmasked[0]['sequence'].split(PADDING)[1],\n",
    "                              sample[\"headline\"])['rouge1_fmeasure']\n",
    "\n",
    "print(score1, score2, score3, score4)\n",
    "plt.imshow(sample[\"image\"])\n",
    "print(\"original headline: \" + sample[\"headline\"], \"section: \" + sample[\"section\"], sep=\"\\n\")\n",
    "print(\"caption: \" + caption[0]['generated_text'], \"masked_sample: \" + masked_sample, \n",
    "      \"unmasked: \" + unmasked[0]['sequence'], \n",
    "      \"prompted_unmasked: \" + prompted_unmasked[0]['sequence'].split(PADDING)[1],\n",
    "      \"section_prompted_unmasked: \" + section_prompted_unmasked[0]['sequence'].split(PADDING)[2],\n",
    "      \"desc_prompted_unmasked: \" + desc_prompted_unmasked[0]['sequence'].split(PADDING)[2],\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# What percentage improves with adding section as a prompt as well?\n",
    "\n",
    "train_val_subset = Subset(train_val_set, range(100))\n",
    "rouge1_fmeasure_prompted = []\n",
    "rouge1_fmeasure_unprompted = []\n",
    "rouge1_fmeasure_section_prompted = []\n",
    "rouge1_fmeasure_desc_prompted = []\n",
    "for data in tqdm(train_val_subset):\n",
    "    caption = captioner(data[\"image\"], max_new_tokens=25)\n",
    "    \n",
    "    masked_sample = randomMask(data[\"headline\"])\n",
    "    \n",
    "    unmasked = unmasker(masked_sample)\n",
    "    prompted_unmasked = unmasker(caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "    section_prompted_unmasked = unmasker(data[\"section\"] + PADDING +\n",
    "                                            caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "    desc_prompted_unmasked = unmasker(\"The context for the caption is {0}.\".format(data[\"section\"]) + PADDING +\n",
    "                                            caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "    rouge1_fmeasure_prompted.append(rouge(prompted_unmasked[0]['sequence'].split(PADDING)[1], \n",
    "                                          data[\"headline\"])['rouge1_fmeasure'])\n",
    "    rouge1_fmeasure_unprompted.append(rouge(unmasked[0]['sequence'],\n",
    "                                            data[\"headline\"])['rouge1_fmeasure'])\n",
    "    rouge1_fmeasure_section_prompted.append(rouge(section_prompted_unmasked[0]['sequence'].split(PADDING)[2],\n",
    "                                            data[\"headline\"])['rouge1_fmeasure'])\n",
    "    rouge1_fmeasure_desc_prompted.append(rouge(desc_prompted_unmasked[0]['sequence'].split(PADDING)[2],\n",
    "                                            data[\"headline\"])['rouge1_fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "rouge1_fmeasure_prompted = np.array(rouge1_fmeasure_prompted)\n",
    "rouge1_fmeasure_unprompted = np.array(rouge1_fmeasure_unprompted)\n",
    "rouge1_fmeasure_section_prompted = np.array(rouge1_fmeasure_section_prompted)\n",
    "rouge1_fmeasure_desc_prompted = np.array(rouge1_fmeasure_desc_prompted)\n",
    "\n",
    "print(\"prompted mean: \", np.mean(rouge1_fmeasure_prompted), \"\\n\",\n",
    "      \"prompted std: \", np.std(rouge1_fmeasure_prompted), \"\\n\",\n",
    "      \"unprompted mean: \", np.mean(rouge1_fmeasure_unprompted), \"\\n\",\n",
    "      \"unprompted std: \", np.std(rouge1_fmeasure_unprompted), \"\\n\",\n",
    "      \"section_prompted mean: \", np.mean(rouge1_fmeasure_section_prompted), \"\\n\",\n",
    "      \"section_prompted std: \", np.std(rouge1_fmeasure_section_prompted), \"\\n\",\n",
    "      \"desc_prompted mean: \", np.mean(rouge1_fmeasure_desc_prompted), \"\\n\",\n",
    "      \"desc_prompted std: \", np.std(rouge1_fmeasure_desc_prompted))\n",
    "print(\"prompted > unprompted: \", np.mean(rouge1_fmeasure_prompted > rouge1_fmeasure_unprompted) - \n",
    "      np.mean(rouge1_fmeasure_prompted < rouge1_fmeasure_unprompted), \"\\n\",\n",
    "      \"section_prompted > unprompted: \", np.mean(rouge1_fmeasure_section_prompted > rouge1_fmeasure_unprompted) - \n",
    "      np.mean(rouge1_fmeasure_section_prompted < rouge1_fmeasure_unprompted), \"\\n\",\n",
    "      \"prompted > section_prompted: \", np.mean(rouge1_fmeasure_prompted > rouge1_fmeasure_section_prompted) - \n",
    "      np.mean(rouge1_fmeasure_prompted < rouge1_fmeasure_section_prompted), \"\\n\",\n",
    "      \"desc_prompted > unprompted: \", np.mean(rouge1_fmeasure_desc_prompted > rouge1_fmeasure_unprompted) -\n",
    "      np.mean(rouge1_fmeasure_desc_prompted < rouge1_fmeasure_unprompted), \"\\n\",\n",
    "      \"desc_prompted > prompted: \", np.mean(rouge1_fmeasure_desc_prompted > rouge1_fmeasure_prompted) -\n",
    "      np.mean(rouge1_fmeasure_desc_prompted < rouge1_fmeasure_prompted), \"\\n\",\n",
    "      \"desc_prompted > section_prompted: \", np.mean(rouge1_fmeasure_desc_prompted > rouge1_fmeasure_section_prompted) -\n",
    "      np.mean(rouge1_fmeasure_desc_prompted < rouge1_fmeasure_section_prompted))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "- Adding descriptive section as a prompt sometime shows improvements over unprompted and prompted.\n",
    "\n",
    "Need to check what works better on what kind of random masking\n",
    "\n",
    "Can we just take all of them and ensemble them somehow?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLIP conditional generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sample BLIP\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(device)\n",
    "\n",
    "img_url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert('RGB')\n",
    "plt.imshow(raw_image)\n",
    "\n",
    "# conditional image captioning\n",
    "text = \"a photography of\"\n",
    "inputs = processor(raw_image, text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=64)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# unconditional image captioning\n",
    "inputs = processor(raw_image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=64)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "\n",
    "# Masked image caption\n",
    "inputs = processor(raw_image, \"woman sitting on the beach with her [MASK]\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "out = model.generate(**inputs, max_new_tokens=64)\n",
    "print(processor.decode(out[0], skip_special_tokens=True))\n",
    "generated_text = processor.batch_decode(out, skip_special_tokens=True)[0].strip()\n",
    "print(generated_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: Seems to complete the sentence but not able to fill the masks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetuning BLIP\n",
    "\n",
    "Finetunig BLIP with MLM Head like Deberta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BLIPFor MLM from scratch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.blip.modeling_blip import BlipPreTrainedModel, BlipModel\n",
    "# from transformers.models.deberta.modeling_deberta import DebertaOnlyMLMHead\n",
    "# from typing import Optional, Union, Tuple\n",
    "# from torch.nn import CrossEntropyLoss, Module\n",
    "# from transformers.modeling_outputs import MaskedLMOutput\n",
    "\n",
    "# class BLIPForMLM(BlipPreTrainedModel):\n",
    "#     _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "#     _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n",
    "\n",
    "#     def __init__(self, config):\n",
    "#         super().__init__(config)\n",
    "\n",
    "#         self.deberta = BlipModel(config)\n",
    "#         self.cls = BlipTextOnlyMLMHead(config)\n",
    "\n",
    "#         # Initialize weights and apply final processing\n",
    "#         self.post_init()\n",
    "\n",
    "#     def get_output_embeddings(self):\n",
    "#         return self.cls.predictions.decoder\n",
    "\n",
    "#     def set_output_embeddings(self, new_embeddings):\n",
    "#         self.cls.predictions.decoder = new_embeddings\n",
    "        \n",
    "#     def forward(\n",
    "#         self,\n",
    "#         input_ids: Optional[torch.Tensor] = None,\n",
    "#         attention_mask: Optional[torch.Tensor] = None,\n",
    "#         token_type_ids: Optional[torch.Tensor] = None,\n",
    "#         position_ids: Optional[torch.Tensor] = None,\n",
    "#         inputs_embeds: Optional[torch.Tensor] = None,\n",
    "#         labels: Optional[torch.Tensor] = None,\n",
    "#         output_attentions: Optional[bool] = None,\n",
    "#         output_hidden_states: Optional[bool] = None,\n",
    "#         return_dict: Optional[bool] = None,\n",
    "#     ) -> Union[Tuple, MaskedLMOutput]:\n",
    "#         r\"\"\"\n",
    "#         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "#             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n",
    "#             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n",
    "#             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n",
    "#         \"\"\"\n",
    "\n",
    "#         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "#         outputs = self.deberta(\n",
    "#             input_ids,\n",
    "#             attention_mask=attention_mask,\n",
    "#             token_type_ids=token_type_ids,\n",
    "#             position_ids=position_ids,\n",
    "#             inputs_embeds=inputs_embeds,\n",
    "#             output_attentions=output_attentions,\n",
    "#             output_hidden_states=output_hidden_states,\n",
    "#             return_dict=return_dict,\n",
    "#         )\n",
    "\n",
    "#         sequence_output = outputs[0]\n",
    "#         prediction_scores = self.cls(sequence_output)\n",
    "\n",
    "#         masked_lm_loss = None\n",
    "#         if labels is not None:\n",
    "#             loss_fct = CrossEntropyLoss()  # -100 index = padding token\n",
    "#             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "\n",
    "#         if not return_dict:\n",
    "#             output = (prediction_scores,) + outputs[1:]\n",
    "#             return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
    "\n",
    "#         return MaskedLMOutput(\n",
    "#             loss=masked_lm_loss,\n",
    "#             logits=prediction_scores,\n",
    "#             hidden_states=outputs.hidden_states,\n",
    "#             attentions=outputs.attentions,\n",
    "#         )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = \"a picture of a [MASK] and her dog on the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "# print(\"inputs: \", processor.decode(inputs['input_ids'][0]))\n",
    "\n",
    "# Training\n",
    "# for p in model.parameters():\n",
    "#     p.requires_grad = True\n",
    "from tqdm import trange\n",
    "for i in trange(100):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "    labels = processor(text=\"a picture of a woman and her dog on the beach\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "    inputs[\"labels\"] = labels\n",
    "    outputs = model(**inputs)\n",
    "    loss = outputs.loss\n",
    "    print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "# print(processor.decode(outputs[0], skip_special_tokens=True))\n",
    "# outputs = model(**inputs)\n",
    "# # print(processor.decode(outputs.decoder_logits[0,0,:]))\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "print(processor.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "outputs.decoder_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "mask_token_index = (inputs.input_ids == processor.tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.decoder_logits\n",
    "print(inputs.input_ids.shape, logits.shape, mask_token_index)\n",
    "print(processor.decode(inputs.input_ids[0,mask_token_index]))\n",
    "predicted_token_id = logits[0, mask_token_index-1].argmax(axis=-1)\n",
    "print(processor.decode(predicted_token_id))\n",
    "print(processor.decode(inputs.input_ids[0]))\n",
    "print(processor.decode(logits[0].argmax(axis=-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation: BLIPForMLM is based on BERT but they shift the output to predict next word. Hence the masked word output in shifted as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "text = \"a [MASK] of a [MASK] and her [MASK] on the [MASK]\"\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "mask_token_index = (inputs.input_ids == processor.tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.decoder_logits\n",
    "print(inputs.input_ids.shape, logits.shape, mask_token_index)\n",
    "print(processor.decode(inputs.input_ids[0,mask_token_index]))\n",
    "predicted_token_id = logits[0, mask_token_index-1].argmax(axis=-1)\n",
    "print(processor.decode(predicted_token_id))\n",
    "print(processor.decode(inputs.input_ids[0]))\n",
    "print(processor.decode(logits[0].argmax(axis=-1)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save a Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_train_set), len(train_val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "randomMask(train_train_set[0]['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchedRandomMask(examples):\n",
    "    headlines = [randomMask(headline) for headline in examples['headline']]\n",
    "    return {'masked_headline': headlines}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "chunked_dataset = train_set.map(batchedRandomMask, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "rindex = random.randint(0, len(chunked_dataset))\n",
    "chunked_dataset[rindex]['masked_headline'], chunked_dataset[rindex]['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "chunked_dataset.save_to_disk(\"./processed_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLIP Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, BlipForConditionalGeneration\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = \"a picture of a [MASK] and her [MASK] on the beach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "def unmask(text, image, device):\n",
    "    inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "    mask_token_index = (inputs.input_ids == processor.tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.decoder_logits\n",
    "    predicted_token_id = logits[0, mask_token_index-1].argmax(axis=-1)\n",
    "    return processor.decode(predicted_token_id)\n",
    "\n",
    "result = unmask(text, image, device)\n",
    "print(result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "all_masked_words = []\n",
    "# val_loader = DataLoader(train_val_set, batch_size=10, shuffle=False)\n",
    "# train_val_subset = torch.utils.data.Subset(train_val_set, range(100))\n",
    "for each_dict in tqdm(train_val_set):\n",
    "    original = each_dict['headline'].split(' ')  # Get the original sentence from the dictionary\n",
    "    sentence = each_dict['masked_headline']  # Get the sentence from the dictionary\n",
    "    image_id = each_dict['image_id']  # Get the image_id from the dictionary\n",
    "    image = each_dict['image']  # Get the image from the dictionary\n",
    "    if \"[MASK]\" in sentence: # See if it has a [MASK] in headline\n",
    "        result = unmask(sentence, image, device).split(' ')\n",
    "\n",
    "        # Make a list of indices where [MASK] is present in the sentence\n",
    "        # If there are more than one [MASK] in the sentence, then add them as separate entries in the result list\n",
    "        indices = [i for i, x in enumerate(sentence.split()) if x == \"[MASK]\"]\n",
    "        if len(indices) > 1:\n",
    "            masked_word_idx_list = []\n",
    "            for i, each_result in enumerate(result):\n",
    "                # Get the top scoring word\n",
    "                all_masked_words.append([image_id, indices[i], each_result, original[indices[i]]])\n",
    "        else:\n",
    "            all_masked_words.append([image_id, indices[0], result[0], original[indices[0]]])\n",
    "\n",
    "final_masked_words = [l[0] for l in all_masked_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "all_masked_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Compute cosine similarity using en_core_web_lg\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "cosine_sim_threshold = 0.5\n",
    "num_correct = 0\n",
    "for each_masked_word in tqdm(all_masked_words):\n",
    "    # Get the top scoring word\n",
    "    top_word = each_masked_word[2]\n",
    "    # Get the original word\n",
    "    original_word = each_masked_word[3]\n",
    "    # Compute cosine similarity\n",
    "    similarity = nlp(top_word).similarity(nlp(original_word))\n",
    "    if similarity >= cosine_sim_threshold:\n",
    "        num_correct += 1\n",
    "\n",
    "accuracy = num_correct / len(all_masked_words) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finetune BLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import pil_to_tensor\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "\n",
    "def custom_collate(original_batch):\n",
    "    filtered_data = {}\n",
    "\n",
    "    for item in original_batch:\n",
    "        image, text = item['image'], item['masked_headline']\n",
    "        original = item['headline']\n",
    "        inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "        inputs[\"labels\"] = processor(text=original, return_tensors=\"pt\").input_ids\n",
    "        for k, v in inputs.items():\n",
    "            if k not in filtered_data:\n",
    "                filtered_data[k] = []\n",
    "            filtered_data[k].append(v)\n",
    "    \n",
    "    # for k, v in filtered_data.items():\n",
    "    #     filtered_data[k] = default_collate(v)\n",
    "\n",
    "    return filtered_data\n",
    "    # return default_collate(filtered_data), default_collate(filtered_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "temp = Subset(train_train_set, range(100))\n",
    "temp_loader = DataLoader(temp, batch_size=10, shuffle=False, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "for i, batch in enumerate(temp_loader):\n",
    "    print(batch.keys(), len(batch['input_ids']), len(batch['labels']))\n",
    "    break\n",
    "    # print(batch['image'].shape, batch['image_id'], batch['headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "from tqdm import trange\n",
    "\n",
    "temp = Subset(train_train_set, range(100))\n",
    "# temp_loader = DataLoader(temp, batch_size=10, shuffle=False, collate_fn=custom_collate)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "losses = []\n",
    "epochs = 10\n",
    "for epoch in trange(epochs):\n",
    "    for i, data in enumerate(tqdm(temp, leave=False)):\n",
    "        model.train()\n",
    "        image, text = data['image'], data['masked_headline']\n",
    "        original = data['headline']\n",
    "        inputs = processor(images=image, text=text, return_tensors=\"pt\")\n",
    "        labels = processor(text=original, return_tensors=\"pt\").input_ids\n",
    "\n",
    "        inputs[\"labels\"] = labels\n",
    "        if inputs['input_ids'].shape != labels.shape:\n",
    "            # print(\"Skipping\")\n",
    "            continue\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f\"Loss: {np.mean(losses)}\")\n",
    "            losses = []\n",
    "\n",
    "# outputs = model.generate(**inputs, max_new_tokens=64)\n",
    "# print(processor.decode(outputs[0], skip_special_tokens=True))\n",
    "# outputs = model(**inputs)\n",
    "# # print(processor.decode(outputs.decoder_logits[0,0,:]))\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "url = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/demo.jpg' \n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "text = \"a picture of a [MASK] and her [MASK] on the beach\"\n",
    "\n",
    "result = unmask(text, image, device)\n",
    "print(text, result, sep='\\n')\n",
    "\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "mask_token_index = (inputs.input_ids == processor.tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.decoder_logits\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "print(processor.decode(logits.argmax(axis=-1)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "all_masked_words = []\n",
    "# val_loader = DataLoader(train_val_set, batch_size=10, shuffle=False)\n",
    "# train_val_subset = torch.utils.data.Subset(train_val_set, range(100))\n",
    "for each_dict in tqdm(train_val_set):\n",
    "    original = each_dict['headline'].split(' ')  # Get the original sentence from the dictionary\n",
    "    sentence = each_dict['masked_headline']  # Get the sentence from the dictionary\n",
    "    image_id = each_dict['image_id']  # Get the image_id from the dictionary\n",
    "    image = each_dict['image']  # Get the image from the dictionary\n",
    "    if \"[MASK]\" in sentence: # See if it has a [MASK] in headline\n",
    "        result = unmask(sentence, image, device).split(' ')\n",
    "\n",
    "        # Make a list of indices where [MASK] is present in the sentence\n",
    "        # If there are more than one [MASK] in the sentence, then add them as separate entries in the result list\n",
    "        indices = [i for i, x in enumerate(sentence.split()) if x == \"[MASK]\"]\n",
    "        if len(indices) > 1:\n",
    "            masked_word_idx_list = []\n",
    "            for i, each_result in enumerate(result):\n",
    "                # Get the top scoring word\n",
    "                all_masked_words.append([image_id, indices[i], each_result, original[indices[i]]])\n",
    "        else:\n",
    "            all_masked_words.append([image_id, indices[0], result[0], original[indices[0]]])\n",
    "\n",
    "final_masked_words = [l[0] for l in all_masked_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "\n",
    "# Compute cosine similarity using en_core_web_lg\n",
    "import spacy\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "cosine_sim_threshold = 0.5\n",
    "num_correct = 0\n",
    "for each_masked_word in tqdm(all_masked_words):\n",
    "    # Get the top scoring word\n",
    "    top_word = each_masked_word[2]\n",
    "    # Get the original word\n",
    "    original_word = each_masked_word[3]\n",
    "    # Compute cosine similarity\n",
    "    similarity = nlp(top_word).similarity(nlp(original_word))\n",
    "    if similarity >= cosine_sim_threshold:\n",
    "        num_correct += 1\n",
    "\n",
    "accuracy = num_correct / len(all_masked_words) * 100\n",
    "print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "- [Done] Create a masked dataset and save it to run tests\n",
    "- [Failed] FineTune the BlipForConditionalGeneration on train dataset\n",
    "    - Run training validation on worst performing examples\n",
    "\n",
    "Maybe:\n",
    "- Train a BLIPMLM Head"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
