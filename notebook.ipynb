{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import pandas as pd\n",
    "import os, json\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"generative-newsai/news-unmasked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = dataset[\"train\"]\n",
    "test_set = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.__len__(), test_set.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.features, test_set.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set[random.choice(range(test_set.__len__()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many test headlines have [MASK] in them?\n",
    "mask_token = \"[MASK]\"\n",
    "test_with_mask = [x for x in test_set[\"headline\"] if mask_token in x]\n",
    "print(f\"{len(test_with_mask)} out of {len(test_set)} test headlines have [MASK] in them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tests without [MASK] in them\n",
    "test_without_mask = [x for x in test_set[\"headline\"] if mask_token not in x]\n",
    "print(f\"{len(test_without_mask)} out of {len(test_set)} test headlines do not have [MASK] in them\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Headlines with no mask in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_without_mask[random.choice(range(len(test_without_mask)))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = np.unique(test_set[\"section\"])\n",
    "print(\"Unique sections in the test dataset: \", len(sections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: We have image, section and a masked headline. We need to predict the appropriate word for the masked headline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many train headlines have [MASK] in them?\n",
    "train_with_mask = [x for x in train_set[\"headline\"] if mask_token in x]\n",
    "print(f\"{len(train_with_mask)} out of {len(train_set)} train headlines have [MASK] in them\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = np.unique(train_set[\"section\"])\n",
    "print(\"Unique sections in the train dataset: \", len(sections))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: Not all the test entries have [MASK] token in them and there are 24 unique sections in the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot section count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot np unique with counts\n",
    "z_train = np.unique(train_set[\"section\"], return_counts=True)\n",
    "z_test = np.unique(test_set[\"section\"], return_counts=True)\n",
    "\n",
    "# np unique returns sorted unique elements\n",
    "assert np.array_equal(z_train[0], z_test[0])\n",
    "\n",
    "# plot grouped bar chart\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(z_train[0]))\n",
    "width = 0.35\n",
    "rects1 = ax.bar(x - width / 2, z_train[1], width, label=\"Train\")\n",
    "rects2 = ax.bar(x + width / 2, z_test[1]*(train_set.__len__()/test_set.__len__()), width, \n",
    "                label=\"Scaled Up Test\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Count by Section\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(z_train[0])\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: We have 24 unique sections with Well, Education, Global Business, Your Money, Economy and Automobiles having lowest occurence. Train dataset and test dataset follow similar pattern for section distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot headline length by section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group headline_length_per_section by section\n",
    "from itertools import groupby\n",
    "\n",
    "def groupby_section(dataset):\n",
    "    ln_per_section = list(zip(dataset[\"section\"], [len(headline) for headline in dataset[\"headline\"]]))\n",
    "    ln_per_section = sorted(ln_per_section, key=lambda x: x[0])\n",
    "    ln_per_section = [(key, list(group)) for key, group in groupby(ln_per_section, lambda x: x[0])]\n",
    "    ln_per_section = [(key, [x[1] for x in group]) for key, group in ln_per_section]\n",
    "    ln_stats_per_section = [(key, np.mean(group), np.std(group)) for key, group in ln_per_section]\n",
    "    return ln_stats_per_section\n",
    "\n",
    "train_ln_stats_per_section = groupby_section(train_set)\n",
    "test_ln_stats_per_section = groupby_section(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train_ln_stats_per_section and test_ln_stats_per_section as grouped bar chart with error bars\n",
    "fig, ax = plt.subplots()\n",
    "x = np.arange(len(train_ln_stats_per_section))\n",
    "width = 0.35\n",
    "rects1 = ax.bar(x - width / 2, [x[1] for x in train_ln_stats_per_section], width, \n",
    "                yerr=[x[2] for x in train_ln_stats_per_section], label=\"Train\")\n",
    "rects2 = ax.bar(x + width / 2, [x[1] for x in test_ln_stats_per_section], width, \n",
    "                yerr=[x[2] for x in test_ln_stats_per_section], label=\"Test\")\n",
    "ax.set_ylabel(\"Headline Length\")\n",
    "ax.set_title(\"Headline Length by Section\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([x[0] for x in train_ln_stats_per_section])\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: No outliers in headline length. We can see that the length of the headline is similar across all sections and lies between ~30 to ~80 characters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample images\n",
    "\n",
    "Uncomment to visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for i, section in enumerate(tqdm(sections)):\n",
    "    indices = random.choices(np.where(np.asarray(train_set[\"section\"]) == section)[0], k=5)\n",
    "    imgs.append(train_set[indices][\"image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot images for each section\n",
    "fig, axs = plt.subplots(6, 4, figsize=(20, 20))\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    ax.imshow(imgs[i][random.randint(0, 4)])\n",
    "    ax.set_title(sections[i])\n",
    "    ax.axis(\"off\")\n",
    "plt.subplots_adjust(wspace=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[img.size for img in train_set[random.sample(range(train_set.__len__()), 10)][\"image\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations: Visualized Images have nytimes labeel showing what year it was printed, and sizes vary across images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deberta-Base-Uncased\n",
    "\n",
    "Based on the sample notebook provided --> https://huggingface.co/datasets/generative-newsai/news-unmasked/blob/main/.extras/helper_notebook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mlcorelib/deberta-base-uncased\"\n",
    "unmasker = pipeline('fill-mask', model=model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check first 5 rows of the Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first 5 rows of the test dataset\n",
    "sample_data = test_set[:5]\n",
    "for k, v in sample_data.items():\n",
    "    print(\"-\"*50)\n",
    "    print(k)\n",
    "    print(*v, sep=\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unmask the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "all_masked_words = []\n",
    "test_subset = Subset(test_set, range(100))\n",
    "for each_dict in tqdm(test_subset):\n",
    "    sentence = each_dict['headline']  # Get the sentence from the dictionary\n",
    "    image_id = each_dict['image_id']  # Get the image_id from the dictionary\n",
    "    if \"[MASK]\" in sentence: # See if it has a [MASK] in headline\n",
    "        result = unmasker(sentence)  # Unmask the sentence\n",
    "\n",
    "        # Make a list of indices where [MASK] is present in the sentence\n",
    "        # If there are more than one [MASK] in the sentence, then add them as separate entries in the result list\n",
    "        indices = [i for i, x in enumerate(sentence.split()) if x == \"[MASK]\"]\n",
    "        if len(indices) > 1:\n",
    "            masked_word_idx_list = []\n",
    "            for i, each_result in enumerate(result):\n",
    "                # Get the top scoring word\n",
    "                top_word = each_result[0]['token_str']\n",
    "                all_masked_words.append([image_id, indices[i], top_word])\n",
    "        else:\n",
    "            all_masked_words.append([image_id, indices[0], result[0]['token_str']])\n",
    "\n",
    "final_masked_words = [l[0] for l in all_masked_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print first 5 rows of the masked words list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_masked_words[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the results as a dataframe and print first 5 rows of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results in a dataframe with column name id,token_index,token\n",
    "df = pd.DataFrame(all_masked_words, columns=['id', 'token_index', 'token'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataframe as a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sample_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vision Language Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea: Caption the image\n",
    "Ask LLM caption + mask sentence. Predict masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide train_set in train and validation\n",
    "train_train_set, train_val_set = torch.utils.data.random_split(train_set, \n",
    "                                                               [int(0.8 * len(train_set)),\n",
    "                                                                len(train_set) - int(0.8 * len(train_set))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_train_set), len(train_val_set), len(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Captioning using hf pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VIT-GPT2-COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "\n",
    "captioner = pipeline(\"image-to-text\",model=\"ydshieh/vit-gpt2-coco-en\")\n",
    "captioner(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n",
    "## [{'generated_text': 'two birds are standing next to each other '}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try captioner on one of the images in the validation set\n",
    "rouge = ROUGEScore(use_stemmer=True)\n",
    "sample = train_val_set[random.randint(0, len(train_val_set))]\n",
    "\n",
    "caption = captioner(sample[\"image\"], max_new_tokens=25)\n",
    "rouge_score = rouge(caption[0]['generated_text'], sample[\"headline\"])\n",
    "plt.imshow(sample[\"image\"])\n",
    "print(\"generated_text: \" + caption[0]['generated_text'], \n",
    "      \"section: \" + sample[\"section\"],  \n",
    "      \"headline: \" + sample[\"headline\"], \n",
    "      \"rouge1_fmeasure: \" + str(rouge_score['rouge1_fmeasure']),\n",
    "      \"rouge2_fmeasure: \" + str(rouge_score['rouge2_fmeasure']),\n",
    "      \"rougeL_fmeasure: \" + str(rouge_score['rougeL_fmeasure']),\n",
    "      \"rougeLsum_fmeasure: \" + str(rouge_score['rougeLsum_fmeasure']),\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rouge1_fmeasure across validation subset\n",
    "rouge1_fmeasure = []\n",
    "train_val_subset = Subset(train_val_set, range(100))\n",
    "for data in tqdm(train_val_subset):\n",
    "    caption = captioner(data[\"image\"], max_new_tokens=25)\n",
    "    rouge_score = rouge(caption[0]['generated_text'], data[\"headline\"])\n",
    "    rouge1_fmeasure.append(rouge_score['rouge1_fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rouge1_fmeasure), np.std(rouge1_fmeasure)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLIP image captioning large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "\n",
    "captioner = pipeline(\"image-to-text\",model=\"Salesforce/blip-image-captioning-large\")\n",
    "captioner(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n",
    "## VIT-GPT2 output: ## [{'generated_text': 'two birds are standing next to each other '}]\n",
    "## BLIP output: [{'generated_text': 'there are two parrots that are standing next to each other'}]\n",
    "## Observations: BLIP output seems more detailed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try captioner on one of the images in the validation set\n",
    "rouge = ROUGEScore(use_stemmer=True)\n",
    "sample = train_val_set[random.randint(0, len(train_val_set))]\n",
    "\n",
    "caption = captioner(sample[\"image\"], max_new_tokens=25)\n",
    "rouge_score = rouge(caption[0]['generated_text'], sample[\"headline\"])\n",
    "plt.imshow(sample[\"image\"])\n",
    "print(\"generated_text: \" + caption[0]['generated_text'], \n",
    "      \"section: \" + sample[\"section\"],  \n",
    "      \"headline: \" + sample[\"headline\"], \n",
    "      \"rouge1_fmeasure: \" + str(rouge_score['rouge1_fmeasure']),\n",
    "      \"rouge2_fmeasure: \" + str(rouge_score['rouge2_fmeasure']),\n",
    "      \"rougeL_fmeasure: \" + str(rouge_score['rougeL_fmeasure']),\n",
    "      \"rougeLsum_fmeasure: \" + str(rouge_score['rougeLsum_fmeasure']),\n",
    "      sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rouge1_fmeasure across validation subset\n",
    "rouge1_fmeasure = []\n",
    "train_val_subset = Subset(train_val_set, range(100))\n",
    "for data in tqdm(train_val_subset):\n",
    "    caption = captioner(data[\"image\"], max_new_tokens=25)\n",
    "    rouge_score = rouge(caption[0]['generated_text'], data[\"headline\"])\n",
    "    rouge1_fmeasure.append(rouge_score['rouge1_fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(rouge1_fmeasure), np.std(rouge1_fmeasure)\n",
    "\n",
    "# VIT-GPT2 scores: (0.06309018, 0.07374029)\n",
    "# BLIP scores: (0.06341785, 0.069144554)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "Inference takes longer than VIT-GPT2 (4:47 as compared to 1:09 for inital 100 samples). Eye balling the results, BLIP image captioning large seems to be more specific to the image. VIT-GPT2 seems to be more generic. rouge score is slightly better for BLIP image captioning large for the first 100 samples. Std dev is also lower for BLIP image captioning large.\n",
    "\n",
    "Conclusions:\n",
    "\n",
    "Can try both models and see which gives better results for masked word prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill-Mask Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "unmasker = pipeline(\"fill-mask\", model=\"distilroberta-base\")\n",
    "unmasker(\"Paris is the <mask> of France.\")\n",
    "\n",
    "# [{'score': 0.7, 'sequence': 'Paris is the capital of France.'},\n",
    "# {'score': 0.2, 'sequence': 'Paris is the birthplace of France.'},\n",
    "# {'score': 0.1, 'sequence': 'Paris is the heart of France.'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_val_set[random.randint(0, len(train_val_set))]\n",
    "\n",
    "# replace random word with [MASK]\n",
    "masked_sample = sample[\"headline\"].split()\n",
    "rand_index = random.randint(0, len(masked_sample)-1)\n",
    "print(f\"rand index: {rand_index}, len of headline: {len(masked_sample)}\")\n",
    "\n",
    "masked_sample[rand_index] = \"[MASK]\"\n",
    "masked_sample = \" \".join(masked_sample)\n",
    "sample[\"headline\"], masked_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unmasked = unmasker(masked_sample)\n",
    "\n",
    "# compute rouge score with original headline\n",
    "rouge = ROUGEScore(use_stemmer=True)\n",
    "print(\"original headline: \" + sample[\"headline\"], \"section: \" + sample[\"section\"], sep=\"\\n\")\n",
    "for i in range(len(unmasked)):\n",
    "    rouge_score = rouge(unmasked[i]['sequence'], sample[\"headline\"])\n",
    "    print(\"generated_text: \" + unmasked[i]['sequence'],  \n",
    "          \"rouge1_fmeasure: \" + str(rouge_score['rouge1_fmeasure']),\n",
    "          \"rouge2_fmeasure: \" + str(rouge_score['rouge2_fmeasure']),\n",
    "          \"rougeL_fmeasure: \" + str(rouge_score['rougeL_fmeasure']),\n",
    "          \"rougeLsum_fmeasure: \" + str(rouge_score['rougeLsum_fmeasure']),\n",
    "          sep=\"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill mask pipeline with caption as prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_val_set[random.randint(0, len(train_val_set))]\n",
    "masked_sample = sample[\"headline\"].split()\n",
    "rand_index = random.randint(0, len(masked_sample)-1)\n",
    "masked_sample[rand_index] = \"[MASK]\"\n",
    "masked_sample = \" \".join(masked_sample)\n",
    "sample[\"headline\"], masked_sample\n",
    "\n",
    "PADDING = \"[ pad ]\"\n",
    "caption = captioner(sample[\"image\"], max_new_tokens=25)\n",
    "unmasked = unmasker(masked_sample)\n",
    "prompted_unmasked = unmasker(caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "\n",
    "plt.imshow(sample[\"image\"])\n",
    "print(\"original headline: \" + sample[\"headline\"], \"section: \" + sample[\"section\"], sep=\"\\n\")\n",
    "print(\"caption: \" + caption[0]['generated_text'], \"masked_sample: \" + masked_sample, \n",
    "      \"unmasked: \" + unmasked[0]['sequence'], \n",
    "      \"prompted_unmasked: \" + prompted_unmasked[0]['sequence'].split(PADDING)[1],\n",
    "        sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What percentage improves with prompting?\n",
    "\n",
    "train_val_subset = Subset(train_val_set, range(100))\n",
    "rouge1_fmeasure_prompted = []\n",
    "rouge1_fmeasure_unprompted = []\n",
    "for data in tqdm(train_val_subset):\n",
    "    caption = captioner(data[\"image\"], max_new_tokens=25)\n",
    "    \n",
    "    masked_sample = data[\"headline\"].split()\n",
    "    rand_index = random.randint(0, len(masked_sample)-1)\n",
    "    masked_sample[rand_index] = \"[MASK]\"\n",
    "    masked_sample = \" \".join(masked_sample)\n",
    "    \n",
    "    unmasked = unmasker(masked_sample)\n",
    "    prompted_unmasked = unmasker(caption[0]['generated_text'] + PADDING + masked_sample)\n",
    "    rouge1_fmeasure_prompted.append(rouge(prompted_unmasked[0]['sequence'].split(PADDING)[1], \n",
    "                                          data[\"headline\"])['rouge1_fmeasure'])\n",
    "    rouge1_fmeasure_unprompted.append(rouge(unmasked[0]['sequence'],\n",
    "                                            data[\"headline\"])['rouge1_fmeasure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge1_fmeasure_prompted = np.array(rouge1_fmeasure_prompted)\n",
    "rouge1_fmeasure_unprompted = np.array(rouge1_fmeasure_unprompted)\n",
    "print(np.mean(rouge1_fmeasure_prompted), np.std(rouge1_fmeasure_prompted), \n",
    "      np.mean(rouge1_fmeasure_unprompted), np.std(rouge1_fmeasure_unprompted))\n",
    "print(np.mean(rouge1_fmeasure_prompted > rouge1_fmeasure_unprompted))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "all_masked_words = []\n",
    "test_subset = Subset(test_set, range(100))\n",
    "for each_dict in tqdm(test_subset):\n",
    "    image = each_dict['image']  # Get the image from the dictionary\n",
    "    sentence = each_dict['headline']  # Get the sentence from the dictionary\n",
    "    image_id = each_dict['image_id']  # Get the image_id from the dictionary\n",
    "    if \"[MASK]\" in sentence: # See if it has a [MASK] in headline\n",
    "        caption = captioner(image, max_new_tokens=25)\n",
    "        result = unmasker(caption[0]['generated_text'] + PADDING + sentence)\n",
    "        \n",
    "        # result = prompted_unmasked[0]['sequence'].split(PADDING)[1]\n",
    "\n",
    "        # Make a list of indices where [MASK] is present in the sentence\n",
    "        # If there are more than one [MASK] in the sentence, then add them as separate entries in the result list\n",
    "        indices = [i for i, x in enumerate(sentence.split()) if x == \"[MASK]\"]\n",
    "        if len(indices) > 1:\n",
    "            masked_word_idx_list = []\n",
    "            for i, each_result in enumerate(result):\n",
    "                # Get the top scoring word\n",
    "                top_word = each_result[0]['token_str']\n",
    "                all_masked_words.append([image_id, indices[i], top_word])\n",
    "        else:\n",
    "            all_masked_words.append([image_id, indices[0], result[0]['token_str']])\n",
    "\n",
    "final_masked_words = [l[0] for l in all_masked_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_masked_words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(all_masked_words, columns=['id', 'token_index', 'token'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sample_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
